---
title: "Accessing VSC & Understanding Credits / Quotas"
format: html
---

## ğŸ¯ Goal

Set up a **minimal working development environment** both locally and on VSC (KU Leuven HPC), and install required language models (e.g. Ollama). Ensure that you can run dry tests and verify installations.

---

## ğŸ”‘ Key Questions

- **What folders should I create?**  
  - Create a structured workspace, both locally and on the cluster:
    ```
    ğŸ“ /Users/you/your_project/
      â””â”€â”€ ğŸ“ code/
      â””â”€â”€ ğŸ“ data/
      â””â”€â”€ ğŸ“ models/
      â””â”€â”€ ğŸ“ logs/
    ```

- **Which minimal dependencies do I install?**  
  - On both environments, install:
    - `conda` or `miniconda`
    - `tqdm`, `requests`, `python-dotenv` (as needed per script)
    - `ollama` (via Slurm on VSC)

- **How do I store the prompt template?**  
  - Save it as a separate `.txt` or `.json` file in the `/code` directory.
  - Alternatively, embed it inside your Python code using a Jinja2-style template string.

---

## ğŸ–¥ï¸ Local Setup (Mac / Linux)

### âœ… Installing Miniconda

```bash
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash Miniconda3-latest-Linux-x86_64.sh
source ~/.bashrc
conda --version  # Verify installation
```

### âœ… Create and activate environment

```bash
conda create -n ollama_venv python=3.10 -y
conda activate ollama_venv
```

### âœ… Install required packages

```bash
conda install "tqdm"
pip install python-dotenv
```

### âœ… Create `.env` file

```bash
cd ~/your_project/code
nano .env
```

Add your OpenAI key:
```
OPENAI_API_KEY=sk-your_openai_key_here
```

---

## ğŸ–§ VSC Setup (HPC KU Leuven)

### ğŸ” SSH Key Setup

```bash
ssh-keygen -t ed25519 -C "your_email@domain.com"
# Follow prompts to save key and add password
ssh-copy-id vscXXXX@login.hpc.kuleuven.be
```

Afterward, go to the [firewall page](https://firewall.vscentrum.be) and complete the authorization.

---

### ğŸ—ï¸ Preparing the Environment

1. **Enable conda:**
    ```bash
    source /data/leuven/377/vscXXXX/miniconda3/etc/profile.d/conda.sh
    ```

2. **Activate environment:**
    ```bash
    module load Python/3.10.4-GCCcore-11.3.0
    conda activate ollama_venv
    ```

3. **Install packages:**
    ```bash
    conda install "tqdm"
    ```

4. **Test installation:**
    ```bash
    python -c "from tqdm import tqdm; print('tqdm is installed')"
    ```

---

## ğŸ¤– Downloading & Running Ollama Models (via SLURM)

### ğŸ› ï¸ Steps to Download Ollama Model

```bash
cd /data/leuven/377/vscXXXX/
ls -l ollama_setup.slurm
sbatch ollama_setup.slurm
squeue -u $USER     # Check job status
tail -f ollama_download.out  # Watch live logs
```

---

### ğŸ§ª Running Ollama Server on HPC

```bash
cd $VSC_DATA
/data/leuven/377/vscXXXX/ollama/bin/ollama serve &
export OLLAMA_MODELS=/staging/leuven/stg_00191/ollama_models
/data/leuven/377/vscXXXX/ollama/bin/ollama list  # View installed models
pkill ollama  # Stop the server
```

---

## âœ… Summary

- SSH + Conda + Slurm = functional dev + deployment setup.
- You can now test models either **locally** or **via VSC**.
- Ensure the `.env` is stored securely and not committed to Git.

---

## ğŸ¯ Goal

Learn how to create and submit SLURM jobs on the KU Leuven VSC cluster. This includes:

- Downloading models with Ollama


## ğŸ¤– Template: Downloading a Model with Ollama via SLURM

```bash
#!/bin/bash -l
#SBATCH --cluster=wice
#SBATCH --partition=batch_sapphirerapids
#SBATCH --time=02:00:00
#SBATCH --mem=128G
#SBATCH --account=lp_h_dv_nlp
#SBATCH --output=ollama_download.out

# Set model location and port
export OLLAMA_MODELS=/staging/leuven/stg_00191/ollama_models
export OLLAMA_HOST=127.0.0.1:37712

# Start Ollama server
/data/leuven/377/vsc37712/ollama/bin/ollama serve &

# Wait until server is active
sleep 15

# Pull the desired model
/data/leuven/377/vsc37712/ollama/bin/ollama pull deepseek-r1:70b

# Wait for download to complete
sleep 600
```

### âš¡ How to Submit the Job

```bash
sbatch ollama_setup.slurm
```

---

## ğŸ“ Folder Organization Tip

Organize your files like so:

```
/data/leuven/377/vscXXXX/
â”œâ”€â”€ slurm_scripts/
â”‚   â”œâ”€â”€ run_model.slurm
â”‚   â””â”€â”€ ollama_setup.slurm
â”œâ”€â”€ python_scripts/
â”‚   â””â”€â”€ icd_9cm_symptoms_deepseek70b.py
â”œâ”€â”€ results/
â”‚   â””â”€â”€ *.out
â””â”€â”€ ollama/
```

---

## ğŸ“„ Summary

- Use SLURM scripts to manage resources and control execution.
- Include robust checks to confirm servers like Ollama are active.
- Log outputs for easy debugging.
- Separate model download and inference workflows into distinct scripts.

---

## âœ… Next Step

Test the SLURM scripts with small jobs to verify functionality. Then scale up to full model runs and analysis batches.

## ğŸ§© Next Step

Proceed to testing your local script with Ollama/OpenAI using dummy prompts. Then, optimize for batch runs on VSC.
