[
  {
    "objectID": "vsc_jobs.html",
    "href": "vsc_jobs.html",
    "title": "Best Practices for Running Models on HPC",
    "section": "",
    "text": "Request Only What You Need:\nEstimate resources and adjust based on past jobs.\nOrganize Jobs & Logs:\nUse descriptive job names and structured output folders.\nUse the Right Storage:\nPlace large and temporary files in $VSC_DATA and $VSC_SCRATCH.\nManage Environments:\nUse modules/conda and document dependencies. Version-control your code.\nPromote Reproducibility:\nSave SLURM scripts, configs, and note job parameters.\nDebug Efficiently:\nCheck logs, use interactive/test jobs, and fail fast on errors.\nDocument & Share:\nKeep READMEs and notebooks. Organize scripts, configs, and results.\nClean Up:\nRemove unused data and files regularly.",
    "crumbs": [
      "Best Practices"
    ]
  },
  {
    "objectID": "vsc_setup.html",
    "href": "vsc_setup.html",
    "title": "Accessing VSC & Setting Up for LLM Workflows",
    "section": "",
    "text": "This tutorial will guide you step-by-step through setting up a working environment on the VSC (Vlaams Supercomputer Centrum).\nBy the end, you‚Äôll be ready to run your first SLURM job‚Äîsuch as downloading or serving a large language model (LLM) like Ollama‚Äîfor your research.\nThis tutorial guides you through setting up your working environment on the VSC, using Visual Studio Code for a seamless experience.\nYou‚Äôll learn how to connect remotely, organize your workspace, configure Python, and download models using SLURM.",
    "crumbs": [
      "Environment Setup"
    ]
  },
  {
    "objectID": "vsc_setup.html#goal",
    "href": "vsc_setup.html#goal",
    "title": "Accessing VSC & Understanding Credits / Quotas",
    "section": "",
    "text": "Set up a minimal working development environment both locally and on VSC (KU Leuven HPC), and install required language models (e.g.¬†Ollama). Ensure that you can run dry tests and verify installations.",
    "crumbs": [
      "Environment Setup"
    ]
  },
  {
    "objectID": "vsc_setup.html#key-questions",
    "href": "vsc_setup.html#key-questions",
    "title": "Accessing VSC & Understanding Credits / Quotas",
    "section": "üîë Key Questions",
    "text": "üîë Key Questions\n\nWhat folders should I create?\n\nCreate a structured workspace, both locally and on the cluster:\nüìÅ /Users/you/your_project/\n  ‚îî‚îÄ‚îÄ üìÅ code/\n  ‚îî‚îÄ‚îÄ üìÅ data/\n  ‚îî‚îÄ‚îÄ üìÅ models/\n  ‚îî‚îÄ‚îÄ üìÅ logs/\n\nWhich minimal dependencies do I install?\n\nOn both environments, install:\n\nconda or miniconda\ntqdm, requests, python-dotenv (as needed per script)\nollama (via Slurm on VSC)\n\n\nHow do I store the prompt template?\n\nSave it as a separate .txt or .json file in the /code directory.\nAlternatively, embed it inside your Python code using a Jinja2-style template string.",
    "crumbs": [
      "Environment Setup"
    ]
  },
  {
    "objectID": "vsc_setup.html#local-setup-mac-linux",
    "href": "vsc_setup.html#local-setup-mac-linux",
    "title": "Accessing VSC & Understanding Credits / Quotas",
    "section": "üñ•Ô∏è Local Setup (Mac / Linux)",
    "text": "üñ•Ô∏è Local Setup (Mac / Linux)\n\n‚úÖ Installing Miniconda\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh\nsource ~/.bashrc\nconda --version  # Verify installation\n\n\n‚úÖ Create and activate environment\nconda create -n ollama_venv python=3.10 -y\nconda activate ollama_venv\n\n\n‚úÖ Install required packages\nconda install \"tqdm\"\npip install python-dotenv\n\n\n‚úÖ Create .env file\ncd ~/your_project/code\nnano .env\nAdd your OpenAI key:\nOPENAI_API_KEY=sk-your_openai_key_here",
    "crumbs": [
      "Environment Setup"
    ]
  },
  {
    "objectID": "vsc_setup.html#vsc-setup-hpc-ku-leuven",
    "href": "vsc_setup.html#vsc-setup-hpc-ku-leuven",
    "title": "Accessing VSC & Understanding Credits / Quotas",
    "section": "üñß VSC Setup (HPC KU Leuven)",
    "text": "üñß VSC Setup (HPC KU Leuven)\n\nüîê SSH Key Setup\nssh-keygen -t ed25519 -C \"your_email@domain.com\"\n# Follow prompts to save key and add password\nssh-copy-id vscXXXX@login.hpc.kuleuven.be\nAfterward, go to the firewall page and complete the authorization.\n\n\n\nüèóÔ∏è Preparing the Environment\n\nEnable conda: bash     source /data/leuven/377/vscXXXX/miniconda3/etc/profile.d/conda.sh\nActivate environment: bash     module load Python/3.10.4-GCCcore-11.3.0     conda activate ollama_venv\nInstall packages: bash     conda install \"tqdm\"\nTest installation: bash     python -c \"from tqdm import tqdm; print('tqdm is installed')\"",
    "crumbs": [
      "Environment Setup"
    ]
  },
  {
    "objectID": "vsc_setup.html#downloading-running-ollama-models-via-slurm",
    "href": "vsc_setup.html#downloading-running-ollama-models-via-slurm",
    "title": "Accessing VSC & Understanding Credits / Quotas",
    "section": "ü§ñ Downloading & Running Ollama Models (via SLURM)",
    "text": "ü§ñ Downloading & Running Ollama Models (via SLURM)\n\nüõ†Ô∏è Steps to Download Ollama Model\ncd /data/leuven/377/vscXXXX/\nls -l ollama_setup.slurm\nsbatch ollama_setup.slurm\nsqueue -u $USER     # Check job status\ntail -f ollama_download.out  # Watch live logs\n\n\n\nüß™ Running Ollama Server on HPC\ncd $VSC_DATA\n/data/leuven/377/vscXXXX/ollama/bin/ollama serve &\nexport OLLAMA_MODELS=/staging/leuven/stg_00191/ollama_models\n/data/leuven/377/vscXXXX/ollama/bin/ollama list  # View installed models\npkill ollama  # Stop the server",
    "crumbs": [
      "Environment Setup"
    ]
  },
  {
    "objectID": "vsc_setup.html#summary",
    "href": "vsc_setup.html#summary",
    "title": "Accessing VSC & Understanding Credits / Quotas",
    "section": "‚úÖ Summary",
    "text": "‚úÖ Summary\n\nSSH + Conda + Slurm = functional dev + deployment setup.\nYou can now test models either locally or via VSC.\nEnsure the .env is stored securely and not committed to Git.",
    "crumbs": [
      "Environment Setup"
    ]
  },
  {
    "objectID": "vsc_setup.html#goal-1",
    "href": "vsc_setup.html#goal-1",
    "title": "Accessing VSC & Understanding Credits / Quotas",
    "section": "üéØ Goal",
    "text": "üéØ Goal\nLearn how to create and submit SLURM jobs on the KU Leuven VSC cluster. This includes:\n\nDownloading models with Ollama",
    "crumbs": [
      "Environment Setup"
    ]
  },
  {
    "objectID": "vsc_setup.html#template-downloading-a-model-with-ollama-via-slurm",
    "href": "vsc_setup.html#template-downloading-a-model-with-ollama-via-slurm",
    "title": "Accessing VSC & Setting Up for LLM Workflows",
    "section": "Template: Downloading a Model with Ollama via SLURM",
    "text": "Template: Downloading a Model with Ollama via SLURM\nBelow is a sample SLURM script for downloading an Ollama model.\n#!/bin/bash -l        \n#SBATCH --cluster=wice\n#SBATCH --partition=batch_sapphirerapids\n#SBATCH --time=02:00:00\n#SBATCH --mem=128G\n#SBATCH --account=lp_h_dv_nlp\n#SBATCH --output=ollama_download.out\n\n# the directory where models will be stored\nexport OLLAMA_MODELS=/staging/leuven/stg_00191/ollama_models\n\n# the host and port for the Ollama server\nexport OLLAMA_HOST=127.0.0.1:37712\n\n# the Ollama server in the background\n/data/leuven/377/vscXXXX/ollama/bin/ollama serve &\n\n# wait a short time to ensure the server starts properly\nsleep 15\n\n# download the desired model from Ollama's repository\n/data/leuven/377/vscXXXX/ollama/bin/ollama pull deepseek-r1:70b\n\n# wait extra time to ensure the download completes before the job ends\nsleep 600\nWhat Each Section of the script does:\n\nThe #SBATCH lines set SLURM job options:\n\ncluster: Specifies which HPC cluster to use.\npartition: Chooses the spscific harware or queue/resource pool to be used.\ntime: Sets the maximum run time allowed for the job (HH:MM:SS).\nmemory: Allocates the required amount of RAM (e.g., 128G for 128 gigabytes).\naccount: Defines the billing/project group charged for resources.\noutput log: Specifies the file to write job output and errors for monitoring and debugging.\n\nThe export lines configure environment variables so Ollama knows where to store models and how to serve them.\nThe command /data/leuven/377/vscXXXX/ollama/bin/ollama serve & starts the Ollama server in the background.\nsleep 15 ensures the server has time to start before the model download begins.\n/data/leuven/377/vscXXXX/ollama/bin/ollama pull deepseek-r1:70b downloads the specified model (deepseek-r1:70b).\nThe final sleep 600 keeps the job running long enough for the download to complete, preventing early termination.",
    "crumbs": [
      "Environment Setup"
    ]
  },
  {
    "objectID": "vsc_setup.html#folder-organization-tip",
    "href": "vsc_setup.html#folder-organization-tip",
    "title": "Accessing VSC & Understanding Credits / Quotas",
    "section": "üìÅ Folder Organization Tip",
    "text": "üìÅ Folder Organization Tip\nOrganize your files like so:\n/data/leuven/377/vscXXXX/\n‚îú‚îÄ‚îÄ slurm_scripts/\n‚îÇ   ‚îú‚îÄ‚îÄ run_model.slurm\n‚îÇ   ‚îî‚îÄ‚îÄ ollama_setup.slurm\n‚îú‚îÄ‚îÄ python_scripts/\n‚îÇ   ‚îî‚îÄ‚îÄ icd_9cm_symptoms_deepseek70b.py\n‚îú‚îÄ‚îÄ results/\n‚îÇ   ‚îî‚îÄ‚îÄ *.out\n‚îî‚îÄ‚îÄ ollama/",
    "crumbs": [
      "Environment Setup"
    ]
  },
  {
    "objectID": "vsc_setup.html#summary-1",
    "href": "vsc_setup.html#summary-1",
    "title": "Accessing VSC & Understanding Credits / Quotas",
    "section": "üìÑ Summary",
    "text": "üìÑ Summary\n\nUse SLURM scripts to manage resources and control execution.\nInclude robust checks to confirm servers like Ollama are active.\nLog outputs for easy debugging.\nSeparate model download and inference workflows into distinct scripts.",
    "crumbs": [
      "Environment Setup"
    ]
  },
  {
    "objectID": "vsc_setup.html#next-step",
    "href": "vsc_setup.html#next-step",
    "title": "Accessing VSC & Understanding Credits / Quotas",
    "section": "‚úÖ Next Step",
    "text": "‚úÖ Next Step\nTest the SLURM scripts with small jobs to verify functionality. Then scale up to full model runs and analysis batches.",
    "crumbs": [
      "Environment Setup"
    ]
  },
  {
    "objectID": "vsc_setup.html#next-step-1",
    "href": "vsc_setup.html#next-step-1",
    "title": "Accessing VSC & Understanding Credits / Quotas",
    "section": "üß© Next Step",
    "text": "üß© Next Step\nProceed to testing your local script with Ollama/OpenAI using dummy prompts. Then, optimize for batch runs on VSC.",
    "crumbs": [
      "Environment Setup"
    ]
  },
  {
    "objectID": "vsc_basics.html",
    "href": "vsc_basics.html",
    "title": "Beginner‚Äôs Guide to working with the VSC",
    "section": "",
    "text": "The Vlaams Supercomputer Center (VSC) is a powerful, shared cluster for research.\nBefore you can run jobs or create a project directory, you need to understand:",
    "crumbs": [
      "HPC & LLM Basics"
    ]
  },
  {
    "objectID": "vsc_shoot.html",
    "href": "vsc_shoot.html",
    "title": "Submitting Jobs with SLURM",
    "section": "",
    "text": "At this stage, you have:\n\nDefined your research question (prompting an LLM for symptoms per ICD code)\nSet up your HPC environment on VSC\nDownloaded and prepared your Ollama models\n\nNow, you need to run your analysis efficiently on the VSC cluster using SLURM.\nIn this section, you will:\n\nWrite a SLURM job script for querying the LLM\nSubmit your script as a job to the VSC cluster\nCollect structured outputs for downstream analysis\n\n\n\n1. Write the Python Script\nYou need a Python script that will:\n\nRead ICD codes and disease names from your input data (e.g., a Parquet file)\nPrompt the LLM for each disease to generate symptom lists\nSave the results in JSONL and Parquet formats for easy downstream analysis\n\nUse the template below as a starting point (customize paths, model name, and prompt as needed):\nimport os\nimport pandas as pd\nimport requests\nimport time\nimport logging\nfrom fastparquet import write as fp_write\n\n# ----------- CONFIGURATION -----------\n\n# set the model name you want to use\nMODEL_NAME = \"your-llm-model-name\"\n\n# update these paths for your environment\nPARQUET_PATH = \"/path/to/your/input_data.parquet\"\nRESULTS_DIR = \"/path/to/your/results\"\nos.makedirs(RESULTS_DIR, exist_ok=True)\n\n# output files\nJSON_OUTPUT = os.path.join(RESULTS_DIR, f\"symptom_results_{MODEL_NAME}.json\")\nPARQUET_OUTPUT = os.path.join(RESULTS_DIR, f\"symptom_results_{MODEL_NAME}.parquet\")\nLOG_FILE = os.path.join(RESULTS_DIR, f\"extract_symptoms_{MODEL_NAME}.log\")\n\n# LLM API endpoint\nLLM_API_URL = \"http://localhost:37712/api/generate\"\n\n# list of models and their settings\nLLM_MODELS = [MODEL_NAME]\nLLM_MODEL_SETTINGS = {\n    \"your-llm-model-name\": {\n        \"temperature\": 0.5,\n        \"max_tokens\": 200,\n        \"top_k\": 3,\n        \"top_n\": 1\n    }\n}\n\nlogging.basicConfig(\n    filename=LOG_FILE,\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s'\n)\n\n# ----------- Environment Check -----------\ndef check_environment():\n    if not os.path.exists(PARQUET_PATH):\n        raise FileNotFoundError(f\"Input Parquet file not found at {PARQUET_PATH}\")\n    if not os.path.exists(RESULTS_DIR):\n        os.makedirs(RESULTS_DIR)\n        logging.info(f\"Created results directory at {RESULTS_DIR}\")\n\n# ----------- Data Loading -----------\ndef load_data(parquet_path):\n    import duckdb\n    conn = duckdb.connect()\n    query = \"\"\"\n        SELECT DISTINCT icd_code, long_title\n        FROM parquet_scan('{}')\n        WHERE icd_code IS NOT NULL AND long_title IS NOT NULL\n    \"\"\".format(parquet_path)\n    df = conn.execute(query).fetch_df()\n    conn.close()\n    return df\n\n# ----------- Prompt Creation -----------\ndef create_prompt(icd_code, description):\n    return (\n        f\"Given the ICD-9CM code {icd_code} with description \\\"{description}\\\", \"\n        f\"identify the 10 most common symptoms typically associated with this condition. \"\n        f\"Return the symptoms as a comma-separated list, and only include the symptom names.\"\n    )\n\n# ----------- LLM Query Function -----------\ndef query_llm(prompt, model_name, temperature=0.5, max_tokens=150, top_k=3, top_n=1):\n    payload = {\n        \"model\": model_name,\n        \"prompt\": prompt,\n        \"temperature\": temperature,\n        \"max_tokens\": max_tokens,\n        \"top_k\": top_k,\n        \"top_n\": top_n,\n        \"stream\": False\n    }\n    try:\n        response = requests.post(LLM_API_URL, json=payload, timeout=60)\n        response.raise_for_status()\n        return response.json().get(\"response\", \"\").strip()\n    except Exception as e:\n        logging.error(f\"LLM request failed for {model_name}: {e}\")\n        return \"\"\n\n# ----------- Main Extraction -----------\ndef extract_symptoms(parquet_path):\n    df = load_data(parquet_path)\n    results = []\n\n    for model_name in LLM_MODELS:\n        logging.info(f\"Processing model: {model_name}\")\n        params = LLM_MODEL_SETTINGS[model_name]\n        for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Model {model_name}\"):\n            icd_code = row[\"icd_code\"]\n            description = row[\"long_title\"]\n            prompt = create_prompt(icd_code, description)\n            start = time.time()\n            response = query_llm(\n                prompt, model_name,\n                temperature=params[\"temperature\"],\n                max_tokens=params[\"max_tokens\"],\n                top_k=params[\"top_k\"],\n                top_n=params[\"top_n\"]\n            )\n            duration = time.time() - start\n            results.append({\n                \"icd_code\": icd_code,\n                \"title\": description,\n                \"model\": model_name,\n                \"response\": response,\n                \"duration\": duration\n            })\n\n    return pd.DataFrame(results)\n\n# ----------- Main Execution -----------\nif __name__ == \"__main__\":\n    try:\n        check_environment()\n        logging.info(\"Starting symptom extraction process...\")\n        start_time = time.time()\n\n        symptom_df = extract_symptoms(PARQUET_PATH)\n        symptom_df.to_json(JSON_OUTPUT, orient=\"records\", indent=2)\n        fp_write(PARQUET_OUTPUT, symptom_df, write_index=False)\n\n        elapsed = time.time() - start_time\n        logging.info(f\"Extraction completed in {elapsed:.2f} seconds. Results saved to {RESULTS_DIR}\")\n        print(f\"Extraction completed in {elapsed:.2f} seconds.\")\n\n    except Exception as e:\n        logging.error(f\"Fatal error: {e}\")\n        print(f\"Fatal error: {e}\")\n        raise\n\n\n\n2. Prepare the SLURM Script\nWrite a SLURM script (for example, llm_symptoms.slurm) that:\n\nRequests the required resources on the cluster\nSets up the LLM server and environment\nRuns your Python script\n\nTemplate below is a guide:\n\n#!/bin/bash -l\n\n#SBATCH --job-name=ollama_icd9cm_symptoms\n#SBATCH --cluster=wice\n#SBATCH --partition=gpu_h100\n#SBATCH --gpus-per-node=1\n#SBATCH --time=02:00:00\n#SBATCH --mem=128G\n#SBATCH --account=xxxxx\n#SBATCH --output=/data/leuven/377/vsc37712/results/icd_9cm_symptoms_deepseek70b%j.out\n\n# Set environment variables for Ollama and model storage\nexport OLLAMA_MODELS=/path/to/your/llm_models\nexport OLLAMA_HOST=127.0.0.1:37712\nexport OLLAMA_PORT=`vscXXXXX`\n\n# Move to the data/code directory\ncd /path/to/your/working_directory\n\n# Activate the Python conda environment\nsource /path/to/your/miniconda3/bin/activate your_env_name\n\n# Set GPU usage\nexport CUDA_VISIBLE_DEVICES=0\n\n# Start Ollama server (logging for debugging)\n/path/to/your/llm_server/bin/llm_server_command serve &gt; /path/to/your/results/llm_server_%j.log 2&gt;&1 &\n\n# Wait for Ollama server to be ready (simple loop)\nfor i in {1..15}; do\n    if curl -s http://127.0.0.1:37712/api/tags &gt; /dev/null; then\n        echo \"Ollama server is up!\"\n        break\n    fi\n    sleep 2\ndone\n\n# Double-check server is up before proceeding\nif ! curl -s http://127.0.0.1:xxxx/api/tags &gt; /dev/null; then\n    echo \"Ollama server did not start correctly.\" &gt;&2\n    exit 1\nfi\n\n# Run the Python script\npython /path/to/your/python_scripts/deepseek70b.py\n\necho \"SLURM job finished at $(date)\"\n\n\n\n3. Submit and Monitor Your Job\nSubmit your SLURM job from the terminal:\nsbatch run_llm_symptom_extraction.slurm\nMonitor your job status:\nsqueue -u $USER\nCheck your logs and results in the output directory you specified in your slurm scripts.\n\n\n\n4. Example: Output Result Snapshot\nA typical output from the LLM will look like this:\n[\n  {\n    \"code\": \"250.00\",\n    \"disease\": \"Diabetes mellitus without mention of complication\",\n    \"symptoms\": [\n      \"increased thirst\",\n      \"frequent urination\",\n      \"blurred vision\",\n      \"fatigue\",\n      \"slow-healing sores\",\n      \"unintended weight loss\",\n      \"hunger\",\n      \"dry mouth\",\n      \"headaches\",\n      \"recurrent infections\"\n    ]\n  },\n  {\n    \"code\": \"401.9\",\n    \"disease\": \"Essential hypertension, unspecified\",\n    \"symptoms\": [\n      \"headache\",\n      \"dizziness\",\n      \"nosebleeds\",\n      \"shortness of breath\",\n      \"chest pain\",\n      \"blurred vision\",\n      \"fatigue\",\n      \"nausea\",\n      \"palpitations\",\n      \"confusion\"\n    ]\n  }\n]",
    "crumbs": [
      "Running Jobs"
    ]
  },
  {
    "objectID": "vsc_shoot.html#goal",
    "href": "vsc_shoot.html#goal",
    "title": "Submitting Jobs with SLURM",
    "section": "",
    "text": "Learn how to create and submit SLURM jobs on the KU Leuven VSC cluster. This includes:\n\nWriting SLURM job scripts\nRunning Python code in a job\nDownloading models with Ollama",
    "crumbs": [
      "Running Jobs"
    ]
  },
  {
    "objectID": "vsc_shoot.html#key-questions",
    "href": "vsc_shoot.html#key-questions",
    "title": "Submitting Jobs with SLURM",
    "section": "üîë Key Questions",
    "text": "üîë Key Questions\n\nHow do I write a SLURM job script?\nHow do I run a Python script in a SLURM job?\nWhat does a complete SLURM setup look like for running or downloading models?",
    "crumbs": [
      "Running Jobs"
    ]
  },
  {
    "objectID": "vsc_shoot.html#structure-of-a-slurm-script",
    "href": "vsc_shoot.html#structure-of-a-slurm-script",
    "title": "Submitting Jobs with SLURM",
    "section": "üõ†Ô∏è Structure of a SLURM Script",
    "text": "üõ†Ô∏è Structure of a SLURM Script\nSLURM job scripts are Bash files with #SBATCH directives that specify:\n\nJob name and resources (e.g., time, memory, GPUs)\nOutput logs\nEnvironment setup (e.g., module load, conda activation)\nThe command(s) to execute",
    "crumbs": [
      "Running Jobs"
    ]
  },
  {
    "objectID": "vsc_shoot.html#template-running-a-python-script-using-ollama",
    "href": "vsc_shoot.html#template-running-a-python-script-using-ollama",
    "title": "Submitting Jobs with SLURM",
    "section": "‚úçÔ∏è Template: Running a Python Script Using Ollama",
    "text": "‚úçÔ∏è Template: Running a Python Script Using Ollama\n#!/bin/bash -l\n#SBATCH --job-name=ollama_icd9cm_symptoms\n#SBATCH --cluster=wice\n#SBATCH --partition=gpu_h100\n#SBATCH --gpus-per-node=1\n#SBATCH --time=02:00:00\n#SBATCH --mem=128G\n#SBATCH --account=lp_h_dv_nlp\n#SBATCH --output=/data/leuven/377/vsc37712/results/icd_9cm_symptoms_deepseek70b%j.out\n\n# Set environment variables\nexport OLLAMA_MODELS=/staging/leuven/stg_00191/ollama_models\nexport OLLAMA_HOST=127.0.0.1:37712\nexport OLLAMA_PORT=37712\n\n# Change to working directory\ncd /data/leuven/377/vsc37712/test_data\n\n# Activate conda environment\nsource /data/leuven/377/vsc37712/miniconda3/bin/activate ollama_venv\n\n# Specify GPU usage\nexport CUDA_VISIBLE_DEVICES=0\n\n# Start Ollama server (logs for debugging)\n/data/leuven/377/vsc37712/ollama/bin/ollama serve &gt; /data/leuven/377/vsc37712/results/ollama_server_%j.log 2&gt;&1 &\n\n# Wait for Ollama to be ready\nfor i in {1..15}; do\n    if curl -s http://127.0.0.1:37712/api/tags &gt; /dev/null; then\n        echo \"Ollama server is up!\"\n        break\n    fi\n    sleep 2\ndone\n\nif ! curl -s http://127.0.0.1:37712/api/tags &gt; /dev/null; then\n    echo \"Ollama server did not start correctly.\" &gt;&2\n    exit 1\nfi\n\n# Run Python script\npython /data/leuven/377/vsc37712/python_scripts/icd_9cm_symptoms_deepseek70b.py\n\necho \"SLURM job finished at $(date)\"\n\n‚ö° How to Submit the Job\nsbatch run_ollama_icd9cm.slurm",
    "crumbs": [
      "Running Jobs"
    ]
  },
  {
    "objectID": "vsc_about.html",
    "href": "vsc_about.html",
    "title": "Running LLMs on HPC for Clinical Symptom Extraction",
    "section": "",
    "text": "This project demonstrates a scalable workflow for generating patient-reported symptom lists for medical conditions using open Large Language Models (LLMs) on the Vlaams Supercomputer (VSC).",
    "crumbs": [
      "Project Overview"
    ]
  },
  {
    "objectID": "vsc_about.html#research-objective",
    "href": "vsc_about.html#research-objective",
    "title": "Running LLMs on HPC for Clinical Symptom Extraction",
    "section": "Research Objective",
    "text": "Research Objective\nFor a curated set of diseases (ICD codes) sourced from the MIMIC-IV clinical database, we systematically prompt open LLMs to produce exactly 10 common symptoms per condition. The outputs are saved in structured formats (JSON/Parquet) for downstream clinical data science.\nThe workflow illustrates how to:\n\nSet up and work efficiently on HPC (High-Performance Computing).\nUnderstand credits and resource allocation on VSC.\nConfigure and prepare the compute environment.\nDownload and manage open-source LLMs for clinical tasks.\nOrchestrate compute jobs using SLURM (including ready-to-use script templates)\nProcess LLM prompts and collect results.\nExtract and structure outputs for analysis (JSONL/Parquet).\nApply best practices for running large-scale jobs on HPC\nAddress basic ethical considerations in generative AI for healthcare",
    "crumbs": [
      "Project Overview"
    ]
  },
  {
    "objectID": "vsc_about.html#pedagogical-value",
    "href": "vsc_about.html#pedagogical-value",
    "title": "Running LLMs on HPC for Clinical Symptom Extraction",
    "section": "",
    "text": "Through this workflow, researchers will learn: - HPC job orchestration using SLURM arrays - Batch processing of LLM queries with error handling - Structured output extraction (JSONL/Parquet) - Reproducible research practices - Ethical considerations for clinical NLP",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "vsc_about.html#data-source-mimic-iv",
    "href": "vsc_about.html#data-source-mimic-iv",
    "title": "Running LLMs on HPC for Clinical Symptom Extraction",
    "section": "Data Source: MIMIC-IV",
    "text": "Data Source: MIMIC-IV\nMIMIC-IV (Medical Information Mart for Intensive Care IV) is a publicly available, de-identified database containing detailed records from ICU patients, including: - Clinical notes and documentation - Diagnostic codes (ICD-9/ICD-10) - Laboratory and vital sign measurements - Medication and procedure records\n\nData Access Requirements\nIn order to access the data the following have to be fulfilled:\n\nComplete CITI training (or equivalent human subjects research training)\nObtain institutional approval through PhysioNet\nSign and comply with the official MIMIC-IV Data Use Agreement\n\n\nNote:\nIn this workflow, only disease names and ICD codes were submitted to LLMs. No patient-level or identifying data was used or transmitted.",
    "crumbs": [
      "Project Overview"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Running LLMs on HPC",
    "section": "",
    "text": "This site is a practical guide for deploying, managing, and analyzing large language models (LLMs) using the Vlaams Supercomputer (VSC).\nThis project was completed as part of my internship, under the guidance of Prof.¬†Dr.¬†Dirk Valkenborg and Jimmy Schepers.\nTheir support and mentorship were instrumental in bringing this work to fruition.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#what-youll-find-here",
    "href": "index.html#what-youll-find-here",
    "title": "Running LLMs on HPC",
    "section": "What You‚Äôll Find Here",
    "text": "What You‚Äôll Find Here\n\nProject Overview:\nUnderstand the goals and scope of this project.\nHPC & LLM Basics:\nLearn foundational concepts about high-performance computing and how to run LLMs.\nEnvironment Setup:\nStep-by-step instructions for setting up your VSC environment, downloading models, and managing dependencies.\nRunning Jobs:\nHow to launch SLURM jobs, organize scripts, and capture logs/output.\nBest Practices:\nProven tips for efficient, reproducible, and scalable research on HPC.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#quick-start",
    "href": "index.html#quick-start",
    "title": "Running LLMs on HPC",
    "section": "",
    "text": "Read the Project Overview to understand the workflow.\nSet up your environment following the Environment Setup section.\nSubmit your first SLURM job using the templates provided.\nExplore Best Practices for optimizing your workflow."
  },
  {
    "objectID": "index.html#about-vsc",
    "href": "index.html#about-vsc",
    "title": "Running LLMs on HPC",
    "section": "About VSC",
    "text": "About VSC\nThe Vlaams SuperComputer (VSC) provides powerful computing resources for research and innovation in Flanders.\nLearn more: VSC User Portal",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "vsc_about.html#acknowledgments",
    "href": "vsc_about.html#acknowledgments",
    "title": "Running LLMs on HPC for Clinical Symptom Extraction",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nI would like to express my sincere gratitude to Prof.¬†Dr.¬†Dirk Valkenborg and Jimmy Schepers for their mentorship and support during this internship. Their expertise and encouragement were vital to the success of this work.",
    "crumbs": [
      "Project Overview"
    ]
  },
  {
    "objectID": "vsc_basics.html#where-does-your-data-live",
    "href": "vsc_basics.html#where-does-your-data-live",
    "title": "Beginner‚Äôs Guide to working with the VSC",
    "section": "Where does your data live?",
    "text": "Where does your data live?\n\n$HOME:\n\nFor your code, scripts, software configs, and small data files.\nThis area is backed up and has a storage limit.\nExample path: /user/&lt;vsc_user&gt;\n\n$VSC_DATA:\n\nFor large datasets and important results that need to persist.\nNot backed up by default! Always keep your own backups of essential data.\nExample path: /data/&lt;project&gt;/&lt;vsc_user&gt;\n\n$VSC_SCRATCH:\n\nFor temporary files and fast, intermediate computation results.\nThis space is cleaned up regularly, do not store anything important here.\nExample path: /scratch/&lt;vsc_user&gt;\n\nSee more at VSC Data Storage",
    "crumbs": [
      "HPC & LLM Basics"
    ]
  },
  {
    "objectID": "vsc_basics.html#where-does-your-code-live",
    "href": "vsc_basics.html#where-does-your-code-live",
    "title": "Beginner‚Äôs Guide to working with the VSC",
    "section": "Where does your code live?",
    "text": "Where does your code live?\n\nTypically, you should store scripts and notebooks in your $HOME or $VSC_DATA/scripts/ directory.",
    "crumbs": [
      "HPC & LLM Basics"
    ]
  },
  {
    "objectID": "vsc_basics.html#where-does-your-terminalsession-live",
    "href": "vsc_basics.html#where-does-your-terminalsession-live",
    "title": "Beginner‚Äôs Guide to working with the VSC",
    "section": "Where does your terminal/session live?",
    "text": "Where does your terminal/session live?\n\nWhen you connect with SSH you are on the login node.\nssh &lt;vsc_user&gt;@login.hpc.kuleuven.be\nOnly use the login node for:\n\nSetting up code and environment\nFile transfers (using scp, rsync)\nSubmitting and managing jobs\n\nNever run heavy computation on the login node!\nAll compute tasks must be submitted as jobs.",
    "crumbs": [
      "HPC & LLM Basics"
    ]
  },
  {
    "objectID": "vsc_basics.html#getting-access",
    "href": "vsc_basics.html#getting-access",
    "title": "Beginner‚Äôs Guide to working with the VSC",
    "section": "1. Getting Access",
    "text": "1. Getting Access\nBefore you can use the VSC (Vlaams Supercomputer Center), you must have an account.\nThis section explains who can get access, what you need, and what happens after you apply.\n\nWho can use the VSC?\n\nStudents and researchers at universities or research institutes in Flanders, such as: KU Leuven (@kuleuven.be), UHasselt (@uhasselt.be), UGent (@ugent.be), VUB (@vub.be) and UAntwerpen (@uantwerpen.be)\nYou must use your institutional email address (for example, john.doe@kuleuven.be).\nPersonal emails like @gmail.com or @hotmail.com are not accepted.\nYou must apply through your university, supervisor, or research group.\nPrivate individuals cannot apply directly.\n\n\n\nWhy apply through your institution?\nThe VSC is a shared resource for academic research.\nWhen you apply through your institution: - Your eligibility is checked. - Your use of the cluster is tracked under your research project or group. - You can access shared credits and storage with your team.\n\n\nHow do you apply for a VSC account?\n\nAsk your supervisor or your university‚Äôs IT support for the correct application procedure.\nComplete the online application form (usually provided by your institution).\n\nFill in your institutional email, name, and department.\nInclude your research group or project name. (Your supervisor may need to approve your application.)\n\nWait for your confirmation email.\n\nThis email will include your VSC user ID and instructions for logging in.\n\nExample confirmation email:\n\"Welcome to the VSC! Your user ID is vsc12345. \nYou can log in using this username and your institutional password.\"\nWhat do you get after your account is approved?\n\n\nA VSC user ID (e.g., vsc12345). This is your username for logging in.\nA default shell (usually Bash, which is the command-line environment).\nThe login address of your VSC cluster, for example:\nssh vsc12345@login.hpc.kuleuven.be\n\nThis command connects you to the VSC.\nIf you see a prompt like vsc12345@login.hpc:~$, you are successfully logged in and ready to start.\n\n\nWhat does this mean?\nYou are now connected to the VSC login node, where you can set up your environment and submit jobs.\n\n\n\n\nImportant points to remember\n\nNever share your password or user ID with anyone.\nYour access is always linked to a research project or group. This is how resources like credits and storage are managed.\nIf you need help or have problems with your account, contact your local IT support or the VSC Helpdesk.",
    "crumbs": [
      "HPC & LLM Basics"
    ]
  },
  {
    "objectID": "vsc_basics.html#make-a-new-directory",
    "href": "vsc_basics.html#make-a-new-directory",
    "title": "Beginner‚Äôs Guide to working with the VSC",
    "section": "# Make a new directory",
    "text": "# Make a new directory",
    "crumbs": [
      "HPC & LLM Basics"
    ]
  },
  {
    "objectID": "vsc_setup.html#prerequisites",
    "href": "vsc_setup.html#prerequisites",
    "title": "Accessing VSC & Setting Up for LLM Workflows",
    "section": "1.1 Prerequisites",
    "text": "1.1 Prerequisites\n\nVisual Studio Code (latest version)\nRemote - SSH extension (ms-vscode-remote.remote-ssh)",
    "crumbs": [
      "Environment Setup"
    ]
  },
  {
    "objectID": "vsc_setup.html#ssh-key-access",
    "href": "vsc_setup.html#ssh-key-access",
    "title": "Accessing VSC & Setting Up for LLM Workflows",
    "section": "1.2 SSH Key Access",
    "text": "1.2 SSH Key Access\nYou need to be able to log into VSC via SSH.\nIf you haven‚Äôt set this up, do the following in your local terminal:\nssh-keygen -t ed25519 -C \"your_email@university.be\"\nssh-copy-id vscXXXX@login.hpc.kuleuven.be\n\nReplace vscXXXX with your VSC username.\nAuthorize your key on the VSC firewall.\nTest SSH:\n\nssh vscXXXX@login.hpc.kuleuven.be",
    "crumbs": [
      "Environment Setup"
    ]
  },
  {
    "objectID": "vsc_setup.html#connect-in-vs-code",
    "href": "vsc_setup.html#connect-in-vs-code",
    "title": "Accessing VSC & Setting Up for LLM Workflows",
    "section": "1.3 Connect in VS Code",
    "text": "1.3 Connect in VS Code\n\nOpen VS Code\nPress F1 (or Ctrl+Shift+P)\nType Remote-SSH: Connect to Host‚Ä¶\nEnter:\n\nssh vscXXXX@login.hpc.kuleuven.be\n\nOpen a New Terminal in VS Code (it will run commands directly on VSC)",
    "crumbs": [
      "Environment Setup"
    ]
  },
  {
    "objectID": "vsc_setup.html#installing-miniconda",
    "href": "vsc_setup.html#installing-miniconda",
    "title": "Accessing VSC & Setting Up for LLM Workflows",
    "section": "3.1 Installing Miniconda",
    "text": "3.1 Installing Miniconda\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh\nsource ~/.bashrc\nconda --version",
    "crumbs": [
      "Environment Setup"
    ]
  },
  {
    "objectID": "vsc_setup.html#load-the-python-module",
    "href": "vsc_setup.html#load-the-python-module",
    "title": "Accessing VSC & Setting Up for LLM Workflows",
    "section": "3.2 Load the Python Module",
    "text": "3.2 Load the Python Module\nmodule load Python/3.10.4-GCCcore-11.3.0",
    "crumbs": [
      "Environment Setup"
    ]
  },
  {
    "objectID": "vsc_setup.html#create-and-activate-your-environment",
    "href": "vsc_setup.html#create-and-activate-your-environment",
    "title": "Accessing VSC & Setting Up for LLM Workflows",
    "section": "3.3 Create and Activate Your Environment",
    "text": "3.3 Create and Activate Your Environment\nconda create -n ollama_venv python=3.10 -y\nconda activate ollama_venv",
    "crumbs": [
      "Environment Setup"
    ]
  },
  {
    "objectID": "vsc_setup.html#install-all-neccesary-packages",
    "href": "vsc_setup.html#install-all-neccesary-packages",
    "title": "Accessing VSC & Setting Up for LLM Workflows",
    "section": "3.4 Install all neccesary packages",
    "text": "3.4 Install all neccesary packages\nYou can either use conda or pip for installation purposes.\nconda install \"package\", ...\npip install python-dotenv",
    "crumbs": [
      "Environment Setup"
    ]
  },
  {
    "objectID": "vsc_setup.html#steps-to-download-an-open-source-ollama-model",
    "href": "vsc_setup.html#steps-to-download-an-open-source-ollama-model",
    "title": "Accessing VSC & Setting Up for LLM Workflows",
    "section": "4.1 Steps to Download an Open Source Ollama Model",
    "text": "4.1 Steps to Download an Open Source Ollama Model\n\nNavigate to your project directory:\n\ncd /data/leuven/377/vscXXXX/\n\nCheck that your SLURM script (ollama_setup.slurm) is present:\n\nls -l ollama_setup.slurm\n\nSubmit the SLURM job to download the model:\n\nsbatch ollama_setup.slurm\nThis command queues your job for execution on the cluster.\n\nMonitor your job‚Äôs status:\n\nsqueue -u $USER\n\nWatch live logs to track the download and check for errors:\n\ntail -f ollama_download.out",
    "crumbs": [
      "Environment Setup"
    ]
  },
  {
    "objectID": "vsc_setup.html#template-for-downloading-open-source-ollama-models-via-slurm",
    "href": "vsc_setup.html#template-for-downloading-open-source-ollama-models-via-slurm",
    "title": "Accessing VSC & Setting Up for LLM Workflows",
    "section": "Template for Downloading Open Source Ollama Models via SLURM",
    "text": "Template for Downloading Open Source Ollama Models via SLURM\nBelow is a sample SLURM script for downloading an Ollama model.\n#!/bin/bash -l        \n#SBATCH --cluster=wice\n#SBATCH --partition=batch_sapphirerapids\n#SBATCH --time=02:00:00\n#SBATCH --mem=128G\n#SBATCH --account=xxxxx\n#SBATCH --output=ollama_download.out\n\n# the directory where models will be stored\nexport OLLAMA_MODELS=/staging/leuven/stg_00191/ollama_models\n\n# the host and port for the Ollama server\nexport OLLAMA_HOST=127.0.0.1:xxxxx\n\n# the Ollama server in the background\n/data/leuven/377/vscXXXX/ollama/bin/ollama serve &\n\n# wait a short time to ensure the server starts properly\nsleep 15\n\n# download the desired model from Ollama's repository\n/data/leuven/377/vscXXXX/ollama/bin/ollama pull deepseek-r1:70b\n\n# wait extra time to ensure the download completes before the job ends\nsleep 600\nWhat Each Section of the script does:\n\nThe #SBATCH lines set SLURM job options:\n\ncluster: Specifies which HPC cluster to use.\npartition: Chooses the spscific harware or queue/resource pool to be used.\ntime: Sets the maximum run time allowed for the job (HH:MM:SS).\nmemory: Allocates the required amount of RAM (e.g., 128G for 128 gigabytes).\naccount: Defines the billing/project group charged for resources.\noutput log: Specifies the file to write job output and errors for monitoring and debugging.\n\nThe export lines configure environment variables so Ollama knows where to store models and how to serve them.\nThe command ‚Äô‚Äò/data/leuven/377/vscXXXX/ollama/bin/ollama serve &‚Äô‚Äô starts the Ollama server in the background.\nsleep 15 ensures the server has time to start before the model download begins.\n‚Äô‚Äò/data/leuven/377/vscXXXX/ollama/bin/ollama pull deepseek-r1:70b‚Äô‚Äô downloads the specified model i.e., deepseek-r1:70b\nThe final sleep 600 keeps the job running long enough for the download to complete, preventing early termination.\n\n\n\nNote:\nOnce this process is complete, your model will be fully downloaded and ready for use in your analyses or experiments. If you wish to download multiple models, simply add additional download commands to the same SLURM script, they will be processed in order during the job.",
    "crumbs": [
      "Environment Setup"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Running LLMs on HPC",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nI sincerely thank Prof.¬†Dr.¬†Dirk Valkenborg and Jimmy Schepers for their exceptional mentorship throughout this project.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#project-context",
    "href": "index.html#project-context",
    "title": "Running LLMs on HPC",
    "section": "",
    "text": "This site is a practical guide for deploying, managing, and analyzing large language models (LLMs) using the Vlaams Supercomputer (VSC).\nThis project was completed as part of my internship, under the guidance of Prof.¬†Dr.¬†Dirk Valkenborg and Jimmy Schepers.\nTheir support and mentorship were instrumental in bringing this work to fruition.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "vsc_setup.html#steps",
    "href": "vsc_setup.html#steps",
    "title": "Accessing VSC & Setting Up for LLM Workflows",
    "section": "4.1 Steps",
    "text": "4.1 Steps\n\nNavigate to your project directory:\n\ncd /data/leuven/377/vscXXXX/\n\nCheck that your SLURM script (ollama_setup.slurm) is present:\n\nls -l ollama_setup.slurm\n\nSubmit the SLURM job to download the model:\n\nsbatch ollama_setup.slurm\nThis command queues your job for execution on the cluster.\n\nMonitor your job‚Äôs status:\n\nsqueue -u $USER\n\nWatch live logs to track the download and check for errors:\n\ntail -f ollama_download.out",
    "crumbs": [
      "Environment Setup"
    ]
  },
  {
    "objectID": "vsc_setup.html#template",
    "href": "vsc_setup.html#template",
    "title": "Accessing VSC & Setting Up for LLM Workflows",
    "section": "4.2 Template",
    "text": "4.2 Template\nBelow is a sample SLURM script for downloading an Ollama model.\n#!/bin/bash -l        \n#SBATCH --cluster=wice\n#SBATCH --partition=batch_sapphirerapids\n#SBATCH --time=02:00:00\n#SBATCH --mem=128G\n#SBATCH --account=xxxxx\n#SBATCH --output=ollama_download.out\n\n# the directory where models will be stored\nexport OLLAMA_MODELS=/staging/leuven/stg_00191/ollama_models\n\n# the host and port for the Ollama server\nexport OLLAMA_HOST=127.0.0.1:xxxxx\n\n# the Ollama server in the background\n/data/leuven/377/vscXXXX/ollama/bin/ollama serve &\n\n# wait a short time to ensure the server starts properly\nsleep 15\n\n# download the desired model from Ollama's repository\n/data/leuven/377/vscXXXX/ollama/bin/ollama pull deepseek-r1:70b\n\n# wait extra time to ensure the download completes before the job ends\nsleep 600\nWhat Each Section of the script does:\n\nThe #SBATCH lines set SLURM job options:\n\ncluster: Specifies which HPC cluster to use.\npartition: Chooses the spscific harware or queue/resource pool to be used.\ntime: Sets the maximum run time allowed for the job (HH:MM:SS).\nmemory: Allocates the required amount of RAM (e.g., 128G for 128 gigabytes).\naccount: Defines the billing/project group charged for resources.\noutput log: Specifies the file to write job output and errors for monitoring and debugging.\n\nThe export lines configure environment variables so Ollama knows where to store models and how to serve them.\nThe command ‚Äô‚Äò/data/leuven/377/vscXXXX/ollama/bin/ollama serve &‚Äô‚Äô starts the Ollama server in the background.\nsleep 15 ensures the server has time to start before the model download begins.\n‚Äô‚Äò/data/leuven/377/vscXXXX/ollama/bin/ollama pull deepseek-r1:70b‚Äô‚Äô downloads the specified model i.e., deepseek-r1:70b\nThe final sleep 600 keeps the job running long enough for the download to complete, preventing early termination.\n\n\n\nNote:\nOnce this process is complete, your model will be fully downloaded and ready for use in your analyses or experiments. If you wish to download multiple models, simply add additional download commands to the same SLURM script, they will be processed in order during the job.",
    "crumbs": [
      "Environment Setup"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Running LLMs on HPC",
    "section": "",
    "text": "This site is a practical guide for deploying, managing, and analyzing large language models (LLMs) using the Vlaams Supercomputer (VSC).",
    "crumbs": [
      "Home"
    ]
  }
]